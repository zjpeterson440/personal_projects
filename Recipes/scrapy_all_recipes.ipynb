{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class AllRecipesSpider(CrawlSpider):\n",
    "    name = 'allrecipes'\n",
    "    allowed_domains = ['allrecipes.com']\n",
    "    start_urls = ['http://www.allrecipes.com/']\n",
    "\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=(r'/recipe/',)), callback='parse_item', follow=True),\n",
    "        Rule(LinkExtractor(allow=(r'/instant',)), callback='parse_item', follow=True),\n",
    "        Rule(LinkExtractor(allow=(r'-recipe-',)), callback='parse_item', follow=True),\n",
    "    )\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AllRecipesSpider, self).__init__(*args, **kwargs)\n",
    "        self.seen_urls = set()  # Initialize an empty set to store seen URLs\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        url = response.url\n",
    "        if url not in self.seen_urls:  # Check if the URL has already been seen\n",
    "            self.seen_urls.add(url)  # Add the URL to the set of seen URLs\n",
    "            item = {'url': url}\n",
    "            yield item\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'FEED_FORMAT': 'csv',\n",
    "    'FEED_URI': 'C:/Users/Admin/Desktop/try_me/allrecipes_links.csv',\n",
    "    'LOG_LEVEL': 'DEBUG',\n",
    "    'DEPTH_LIMIT': 7,\n",
    "    'CLOSESPIDER_TIMEOUT': 600  # 30 minutes time limit\n",
    "})\n",
    "\n",
    "process.crawl(AllRecipesSpider)\n",
    "process.start()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('allrecipes_links.csv')\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "def is_recipe(check):\n",
    "    # Check if any of the substrings is in 'check'\n",
    "    if '/recipe/' in check or '-recipe-' in check:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "df['is_recipe'] = df['url'].apply(is_recipe)\n",
    "\n",
    "df = df[df['is_recipe']==True]\n",
    "\n",
    "\n",
    "###################################\n",
    "\n",
    "df2 = df.drop(columns = ['is_recipe'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Chunks (Step 2)\n",
    "step_size = 5000\n",
    "\n",
    "for start in range(0, len(df2), step_size):\n",
    "    end = start + step_size\n",
    "    # Slice the DataFrame\n",
    "    smaller_df = df2.iloc[start:end]\n",
    "    # Save each chunk to a CSV file\n",
    "    filename = f'data_chunk_{start // step_size + 1}.csv'  # Naming each file uniquely\n",
    "    smaller_df.to_csv(filename, index=False)  # Saving the file without the index\n",
    "\n",
    "chunk_number = 1  # The chunk to process\n",
    "filename = f'data_chunk_{chunk_number}.csv'\n",
    "chunk_df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_and_parse(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "    return None\n",
    "\n",
    "def get_cook_info(soup):\n",
    "    recipe_details = []\n",
    "    if soup:\n",
    "        items = soup.find_all('div', class_='mntl-recipe-details__item')\n",
    "        for item in items:\n",
    "            label = item.find('div', class_='mntl-recipe-details__label').get_text(strip=True).rstrip(':')\n",
    "            value = item.find('div', class_='mntl-recipe-details__value').get_text(strip=True)\n",
    "            recipe_details.append({label: value})\n",
    "    return recipe_details\n",
    "\n",
    "def get_ingredients(soup):\n",
    "    ingredients = []\n",
    "    if soup:\n",
    "        containers = soup.find_all('div', class_='comp mntl-structured-ingredients')\n",
    "        for container in containers:\n",
    "            for item in container.find_all('li', class_='mntl-structured-ingredients__list-item'):\n",
    "                quantity = item.find('span', {'data-ingredient-quantity': 'true'}).get_text(strip=True) if item.find('span', {'data-ingredient-quantity': 'true'}) else None\n",
    "                unit = item.find('span', {'data-ingredient-unit': 'true'}).get_text(strip=True) if item.find('span', {'data-ingredient-unit': 'true'}) else None\n",
    "                name = item.find('span', {'data-ingredient-name': 'true'}).get_text(strip=True) if item.find('span', {'data-ingredient-name': 'true'}) else None\n",
    "                ingredients.append({'quantity': quantity, 'unit': unit, 'name': name})\n",
    "    return ingredients\n",
    "\n",
    "def get_nutrition(soup):\n",
    "    nutrition_info = []\n",
    "    if soup:\n",
    "        nutrition_table = soup.find('table', class_='mntl-nutrition-facts-label__table')\n",
    "        if nutrition_table:\n",
    "            servings_header = nutrition_table.find('tr', class_='mntl-nutrition-facts-label__servings')\n",
    "            calories_header = nutrition_table.find('tr', class_='mntl-nutrition-facts-label__calories')\n",
    "            if servings_header:\n",
    "                servings_full_text = servings_header.get_text(separator=\" \", strip=True)\n",
    "                servings_parts = servings_full_text.split()\n",
    "                servings_label = \" \".join(servings_parts[:-1])\n",
    "                servings_number = servings_parts[-1]\n",
    "                nutrition_info.append({servings_label: servings_number})\n",
    "            if calories_header:\n",
    "                calories_full_text = calories_header.get_text(separator=\" \", strip=True)\n",
    "                calories_parts = calories_full_text.split()\n",
    "                calories_label = \" \".join(calories_parts[:-1])\n",
    "                calories_number = calories_parts[-1]\n",
    "                nutrition_info.append({calories_label: calories_number})\n",
    "            for nut in nutrition_table.find_all('td'):\n",
    "                nutrition_full = nut.get_text(separator=\" \", strip=True)\n",
    "                parts = nutrition_full.split()\n",
    "                nut_label = \" \".join(parts[:-1])\n",
    "                nut_number = parts[-1]\n",
    "                nut_dict = {nut_label: nut_number}\n",
    "                if nut_label != '':\n",
    "                    nutrition_info.append(nut_dict)\n",
    "    return nutrition_info\n",
    "\n",
    "def get_ratings(soup):\n",
    "    rating_dict = {}\n",
    "    if soup:\n",
    "        ratings = soup.find('div', class_='comp mntl-recipe-review-bar recipe-review-bar mntl-block has-ratings has-comments js-recipe-review-bar')\n",
    "        if ratings:\n",
    "            overall_rating = ratings.find('div', class_='comp mntl-recipe-review-bar__rating mntl-text-block type--squirrel-bold').get_text(strip=True)\n",
    "            num_ratings = ratings.find('div', class_='comp mntl-recipe-review-bar__rating-count mntl-text-block type--squirrel').get_text(strip=True).replace('(','').replace(')','')\n",
    "            rating_dict = {'rating': overall_rating, 'num_ratings': num_ratings}\n",
    "    return rating_dict if rating_dict else \"No Ratings\"\n",
    "\n",
    "def get_tags(soup):\n",
    "    tags = []\n",
    "    if soup:\n",
    "        location = soup.find('div', class_='loc article-header')\n",
    "        if location:\n",
    "            a_tags = location.find_all('a', href=True)\n",
    "            for a_tag in a_tags:\n",
    "                href = a_tag['href']\n",
    "                if href.startswith('https://'):\n",
    "                    tag = href.rstrip('/').split('/')[-1]\n",
    "                    tags.append(tag)\n",
    "    return tags\n",
    "\n",
    "def get_title(soup):\n",
    "    if soup:\n",
    "        h1_tag = soup.find('h1')\n",
    "        if h1_tag:\n",
    "            return h1_tag.get_text(strip=True)\n",
    "    return \"Error\"\n",
    "\n",
    "def process_data(url):\n",
    "    soup = fetch_and_parse(url)\n",
    "    if not soup:\n",
    "        return {\n",
    "            'title': \"Error\",\n",
    "            'tags': \"Error\",\n",
    "            'ratings': \"Error\",\n",
    "            'nutrition': \"Error\",\n",
    "            'ingredients': \"Error\",\n",
    "            'cook_info': \"Error\"\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'title': get_title(soup),\n",
    "        'tags': get_tags(soup),\n",
    "        'ratings': get_ratings(soup),\n",
    "        'nutrition': get_nutrition(soup),\n",
    "        'ingredients': get_ingredients(soup),\n",
    "        'cook_info': get_cook_info(soup)\n",
    "    }\n",
    "\n",
    "def main(chunk_number):\n",
    "    filename = f'data_chunk_{chunk_number}.csv'\n",
    "    df_chunk = pd.read_csv(filename)\n",
    "    processed_data = [process_data(url) for url in df_chunk['url']]\n",
    "    processed_df = pd.DataFrame(processed_data)\n",
    "    final_df = pd.concat([df_chunk, processed_df], axis=1)\n",
    "    final_df.to_csv(f'final_data_chunk_{chunk_number}.csv', index=False)\n",
    "    return final_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chunk_number = 9  # Set the chunk number you want to process\n",
    "    final_df = main(chunk_number)\n",
    "    print(f\"Processing of chunk {chunk_number} completed. Data saved to 'final_data_chunk_{chunk_number}.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################### SLOW VERSION, DONT USE ############################\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_cook_info(link):\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        recipe_details = []\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            items = soup.find_all('div', class_='mntl-recipe-details__item')\n",
    "            for item in items:\n",
    "                label = item.find('div', class_='mntl-recipe-details__label').get_text(strip=True).rstrip(':')\n",
    "                value = item.find('div', class_='mntl-recipe-details__value').get_text(strip=True)\n",
    "                item_dict = {label: value}\n",
    "                recipe_details.append(item_dict)\n",
    "        return recipe_details\n",
    "    except Exception as e:\n",
    "        return \"Error\"\n",
    "\n",
    "def get_ingredients(link):\n",
    "    try:\n",
    "        ingredients = []\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            ingredients_container = soup.find_all('div', class_='comp mntl-structured-ingredients')\n",
    "            for container in ingredients_container:\n",
    "                for item in container.find_all('li', class_='mntl-structured-ingredients__list-item'):\n",
    "                    quantity = item.find('span', {'data-ingredient-quantity': 'true'}).get_text(strip=True) if item.find('span', {'data-ingredient-quantity': 'true'}) else None\n",
    "                    unit = item.find('span', {'data-ingredient-unit': 'true'}).get_text(strip=True) if item.find('span', {'data-ingredient-unit': 'true'}) else None\n",
    "                    name = item.find('span', {'data-ingredient-name': 'true'}).get_text(strip=True) if item.find('span', {'data-ingredient-name': 'true'}) else None\n",
    "                    ingredient_dict = {'quantity': quantity, 'unit': unit, 'name': name}\n",
    "                    ingredients.append(ingredient_dict)\n",
    "        return ingredients\n",
    "    except Exception as e:\n",
    "        return \"Error\"\n",
    "\n",
    "def get_nutrition(link):\n",
    "    try:\n",
    "        nutrition_info = []\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            nutrition_table = soup.find('table', class_='mntl-nutrition-facts-label__table')\n",
    "            if nutrition_table:\n",
    "                servings_header = nutrition_table.find('tr', class_='mntl-nutrition-facts-label__servings')\n",
    "                calories_header = nutrition_table.find('tr', class_='mntl-nutrition-facts-label__calories')\n",
    "                if servings_header:\n",
    "                    servings_full_text = servings_header.get_text(separator=\" \", strip=True)\n",
    "                    servings_parts = servings_full_text.split()\n",
    "                    servings_label = \" \".join(servings_parts[:-1])\n",
    "                    servings_number = servings_parts[-1]\n",
    "                    nutrition_info.append({servings_label: servings_number})\n",
    "                if calories_header:\n",
    "                    calories_full_text = calories_header.get_text(separator=\" \", strip=True)\n",
    "                    calories_parts = calories_full_text.split()\n",
    "                    calories_label = \" \".join(calories_parts[:-1])\n",
    "                    calories_number = calories_parts[-1]\n",
    "                    nutrition_info.append({calories_label: calories_number})\n",
    "                for nut in nutrition_table.find_all('td'):\n",
    "                    nutrition_full = nut.get_text(separator=\" \", strip=True)\n",
    "                    parts = nutrition_full.split()\n",
    "                    nut_label = \" \".join(parts[:-1])\n",
    "                    nut_number = parts[-1]\n",
    "                    nut_dict = {nut_label: nut_number}\n",
    "                    if nut_label != '':\n",
    "                        nutrition_info.append(nut_dict)\n",
    "        return nutrition_info\n",
    "    except Exception as e:\n",
    "        return \"Error\"\n",
    "\n",
    "def get_ratings(link):\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        rating_dict = {}\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            ratings = soup.find('div', class_='comp mntl-recipe-review-bar recipe-review-bar mntl-block has-ratings has-comments js-recipe-review-bar')\n",
    "            if ratings:\n",
    "                overall_rating = ratings.find('div', class_='comp mntl-recipe-review-bar__rating mntl-text-block type--squirrel-bold').get_text(strip=True)\n",
    "                num_ratings = ratings.find('div', class_='comp mntl-recipe-review-bar__rating-count mntl-text-block type--squirrel').get_text(strip=True).replace('(','').replace(')','')\n",
    "                rating_dict = {'rating': overall_rating, 'num_ratings': num_ratings}\n",
    "        return rating_dict\n",
    "    except Exception as e:\n",
    "        return \"No Ratings\"\n",
    "\n",
    "def get_tags(link):\n",
    "    try:\n",
    "        tags = []\n",
    "        response = requests.get(link)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            location = soup.find('div', class_='loc article-header')\n",
    "            if location:\n",
    "                a_tags = location.find_all('a', href=True)\n",
    "                all_links = [a['href'] for a in a_tags if a['href'].startswith('https://')]\n",
    "                for clean in all_links:\n",
    "                    cleaned = clean[:-1].split('/')[-1]\n",
    "                    tags.append(cleaned)\n",
    "        return tags\n",
    "    except Exception as e:\n",
    "        return \"Error\"\n",
    "\n",
    "def get_title(link):\n",
    "    try:\n",
    "        response = requests.get(link)\n",
    "        rep_title = \"Error\"\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            h1_tag = soup.find('h1')\n",
    "            if h1_tag:\n",
    "                rep_title = h1_tag.get_text(strip=True)\n",
    "        return rep_title\n",
    "    except Exception as e:\n",
    "        return \"Error\"\n",
    "\n",
    "\n",
    "df = pd.read_csv('testing.csv')\n",
    "\n",
    "df['title'] = df['url'].apply(get_title)\n",
    "df['tags'] = df['url'].apply(get_tags)\n",
    "df['ratings'] = df['url'].apply(get_ratings)\n",
    "df['nutrition'] = df['url'].apply(get_nutrition)\n",
    "df['ingredients'] = df['url'].apply(get_ingredients)\n",
    "df['cook_info'] = df['url'].apply(get_cook_info)\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
