{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from twisted.internet import reactor  # Import reactor for setting up a manual timeout\n",
    "\n",
    "class SeriousEatsSpider(CrawlSpider):\n",
    "    name = 'aseriousEats'\n",
    "    allowed_domains = ['seriouseats.com']\n",
    "    start_urls = ['https://www.seriouseats.com/']\n",
    "    request_count = 0  # Initialize request counter\n",
    "\n",
    "    rules = (\n",
    "        # Adjusted rules as per your requirement\n",
    "        Rule(LinkExtractor(deny=(r'/how-to/',)), follow=True, callback='parse_item'),\n",
    "    )\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(SeriousEatsSpider, self).__init__(*args, **kwargs)\n",
    "        self.seen_urls = set()\n",
    "        # Setup a manual timeout using reactor.callLater\n",
    "        reactor.callLater(600, self.close_spider_due_to_timeout)  # Schedule spider to close after 600 seconds\n",
    "\n",
    "    def close_spider_due_to_timeout(self):\n",
    "        self.crawler.engine.close_spider(self, 'timeout_reached')  # Close spider due to timeout\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        self.request_count += 1  # Increment request count for every processed request\n",
    "        if self.request_count > 80000:  # Check if request count exceeds the limit\n",
    "            self.crawler.engine.close_spider(self, 'request_limit_reached')  # Close spider due to request limit reached\n",
    "            return\n",
    "        \n",
    "        url = response.url\n",
    "        if url not in self.seen_urls:\n",
    "            self.seen_urls.add(url)\n",
    "            item = {'url': url}\n",
    "            yield item\n",
    "\n",
    "# Scrapy CrawlerProcess setup\n",
    "process = CrawlerProcess(settings={\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'FEED_FORMAT': 'csv',\n",
    "    'FEED_URI': 'C:/Users/Admin/Desktop/try_me/seriouseats_2.csv',\n",
    "    'LOG_LEVEL': 'DEBUG',\n",
    "    #'DEPTH_LIMIT': 10,\n",
    "})\n",
    "\n",
    "process.crawl(SeriousEatsSpider)\n",
    "process.start()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 19:38:46 [py.warnings] WARNING: C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_10008\\1015732504.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('seriouseats_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.seriouseats.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.seriouseats.com/vegetable-guides-5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.seriouseats.com/pantry-guides-5181287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.seriouseats.com/equipment-5117081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.seriouseats.com/noodle-guides-5117999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url\n",
       "0                       https://www.seriouseats.com/\n",
       "1  https://www.seriouseats.com/vegetable-guides-5...\n",
       "2  https://www.seriouseats.com/pantry-guides-5181287\n",
       "3      https://www.seriouseats.com/equipment-5117081\n",
       "4  https://www.seriouseats.com/noodle-guides-5117999"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#div comp structured-ingredients\n",
    "#https://www.seriouseats.com/bun-cha-hanoi-recipe-8421208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 21:12:27 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): www.seriouseats.com:443\n",
      "2024-02-15 21:12:28 [urllib3.connectionpool] DEBUG: https://www.seriouseats.com:443 \"GET /spaghetti-with-canned-clam-sauce HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4 tablespoons (60 g) unsalted butter , cut into 4 pieces and divided', '2 celery ribs (5 oz; 140 g ), peeled (optional) and cut into small dice', '2 medium shallots ( 60 g ), minced', '1/2 cup ( 30 g ) finely chopped fresh parsley leaves , divided', '4 medium garlic cloves ( 15 g ), finely minced', '1 fresh green Thai chile ( 4 g ), thinly sliced (optional, see note)', 'Freshly ground black pepper', 'Two ( 8-ounce ; 237 ml) bottles clam juice', 'Two (6.5-ounce; 184 g ) cans chopped or minced clams , clams and liquid divided', '1 teaspoon (5 ml) soy sauce', '1 pound ( 450 g ) spaghetti', 'Kosher salt', 'Celery leaves (pale yellow-green leaves from the celery heart), for garnish', 'Lemon wedges , for serving']\n"
     ]
    }
   ],
   "source": [
    "#Done Ingredients\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re  # Import regular expressions\n",
    "\n",
    "url = 'https://www.seriouseats.com/spaghetti-with-canned-clam-sauce'\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    ingredients_list = soup.find_all('li', class_='structured-ingredients__list-item')\n",
    "\n",
    "    formatted_ingredients = []\n",
    "    for item in ingredients_list:\n",
    "        p_tag = item.find('p')\n",
    "        if p_tag:  # Check if the <p> tag exists\n",
    "            ingredient_text = p_tag.get_text(\" \", strip=True)  # Use a space as separator for inner tags\n",
    "            # Use regular expression to find quantities and units and ensure proper spacing\n",
    "            formatted_text = re.sub(r'(\\d+)\\s*(g|ml|ounces|cup|tablespoons|teaspoon|pounds|bunch)', r'\\1 \\2', ingredient_text, flags=re.I)\n",
    "            formatted_text = re.sub(r'(\\d+)([a-zA-Z])', r'\\1 \\2', formatted_text)  # Add space between numbers and text\n",
    "            formatted_ingredients.append(formatted_text)\n",
    "\n",
    "print(formatted_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE Rating\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.seriouseats.com/basque-cheesecake'\n",
    "\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    star_rating = soup.find('div', class_='comp js-feedback-trigger aggregate-star-rating mntl-block')\n",
    "\n",
    "    rating_counter = 0\n",
    "\n",
    "    if star_rating:  # Check if star_rating is not None\n",
    "        # Assuming you want to find all <a> tags with class 'active' or 'half' within star_rating\n",
    "        active_ratings = star_rating.find_all('a', class_='active')\n",
    "        half_ratings = star_rating.find_all('a', class_='half')\n",
    "\n",
    "        rating_counter += len(active_ratings)  # Each 'active' class represents 1 star\n",
    "        rating_counter += 0.5 * len(half_ratings)  # Each 'half' class represents 0.5 star\n",
    "        num_reviews = star_rating.get_text(strip=True).replace('(', '').replace(')','')\n",
    "        overall_rating = [rating_counter, int(num_reviews)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE Title\n",
    "\n",
    "url = 'https://www.seriouseats.com/basque-cheesecake'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "    title = soup.find('h1').get_text(strip=True)\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE COOKING VALUES\n",
    "\n",
    "url = 'https://www.seriouseats.com/basque-cheesecake'\n",
    "response = requests.get(url)\n",
    "\n",
    "recipe_time = []\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # Target the container that directly holds the time and serving information\n",
    "    times_container = soup.find_all('div', class_='project-meta__times-container')\n",
    "    results_container = soup.find_all('div', class_='project-meta__results-container')\n",
    "\n",
    "    # Process each container separately to ensure all relevant data is captured\n",
    "    for container in times_container + results_container:\n",
    "        for items in container.find_all('div', class_='loc'):  # Find all divs with class 'loc' within the container\n",
    "            label = items.find('span', class_='meta-text__label').get_text(strip=True) if items.find('span', class_='meta-text__label') else None\n",
    "            value = items.find('span', class_='meta-text__data').get_text(strip=True) if items.find('span', class_='meta-text__data') else None\n",
    "            if label and value:  # Ensure both label and value are found\n",
    "                recipe_time.append({label: value})\n",
    "\n",
    "print(recipe_time)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUTRITION\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
